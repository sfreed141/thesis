\chapter{Implementation}

At its core, the application is a forward renderer which uses Blinn-Phong shading and supports multiple lights, shadow mapping, and normal mapping. The scene to be rendered is composed of one or more actors, which are simply meshes loaded in from the generic Wavefront OBJ file format. Actors are also capable of rigid body animations.

The rendering pipeline can be broken down into several render passes, each of which will be explained in its corresponding section. In general, the main steps taken are generating the voxelized representation of the scene, creating the filtered representation of light from the voxels, and finally shading the scene.

The application itself is written in C++11 and uses OpenGL as the graphics API. In order to make use of modern graphics features like compute shaders and direct state access we require OpenGL version 4.5 (released in 2014). The project uses CMake as its build system and has been tested on Linux and Windows (macOS only supports up to 4.1).

% TODO include code snippets, API calls, diagrams
%TODO name the 3D texture (and refer to it in monospace)?
\section{Voxelization}
The goal of voxelization is to create a sparse 3D representation of the geometry in the scene. This information is stored in a 3D texture, a uniformly spaced grid of data. Each element in this texture is a voxel which stores an RGBA value representing the color and opacity of the given voxel.

% TODO scene extents, midpoint and tracking, computing voxel texture coordinates, choosing axis, diagram of scene extents and converting to NDC
% TODO cite GPU Pro 4 and imageAtomicMax
The actual implementation utilizes the GPU rasterization pipeline and is based on~\cite{crassin12}. The basic idea is to rasterize the scene such that each fragment generated by the GPU corresponds to a single voxel in the 3D texture. The 3D position of the fragment can then be computed using the depth and pixel coordinates for the fragment. Then, since multiple fragments may be generated for a single destination in the voxel texture, we average the resulting color value using atomic operations~\footnote{Other approaches to this problem (with various tradeoffs) can be used. These are generally used to combat performance issues or hardware limitations}.
% TODO should explain rgba8 + faking atomic avg vs rgba16f + atomicAdd + compute shader

% TODO should expand on problem more
One of the main challenges with voxelization is ensuring there are no holes or cracks in the resulting grid. Two techniques are used to mitigate this. First, for each triangle we perform the projection along the triangle's normal's dominant axis. This maximizes the number of fragments generated. Second, we perform conservative raterization. Normally, the GPU only generates a fragment for a given pixel if the triangle overlaps the center of the corresponding pixel. This results in cracks in the voxelized grid since even if a triangle is inside a voxel it may not be rasterized. To achieve conservative rasterization, we use an MSAA based approach. MSAA (multi-sample anti-aliasing) is a technique used for smoothing out ragged edges (aliasing) in a rasterized image. Instead of only sampling at one position inside the voxel, multiple samples are used (hence the name). These samples are typically distributed to try to maximize the possibility of a rasterized fragment being produced. For each sample point that overlaps with the triangle, a fragment is generated. The final value of the fragment (e.g.\ rgb color value of a pixel) is the averaged value of all the samples. For conservative rasterization, we only need one fragment to be generated, which can be configured with TODO. While the MSAA method of conservative rasterization is not perfect, it is cross-platform, simple to use, and efficient. Other notable methods would be manually dilating each triangle in a geometry shader (better quality but slower) or using a GPU vendor specific extension, like \verb#GL_NV_conservative_raster# CITE (better quality but not cross-platform).

% TODO go through shaders. NV_conservative_raster, MSAA based (CITE THIS), image of MSAA and the sampling, one fragment for msaa

% TODO code snippets, actual OpenGL names
\section{Shadow Mapping}
Shadow mapping is a well known technique used for efficiently creating shadows. We will also use the resulting shadow map for radiance injection.

To generate a shadow map for a light, we render the scene from the light's perspective and gather the depth values of the fragments. Since we need the GPU to write to an arbitrary texture (the shadowmap), we create a framebuffer object (FBO) and attach the texture as a depth attachment. The FBO does not require any color attachments, as we are only interested in the depth. The matrix used to transform vertices from world space to light space is an orthographic projection matrix multiplied with a view matrix generated from the light. Since the GPU will automatically write depth values, the fragment shader can be completely empty. The resulting shadow map, which only contains floating point depth values (in the range [0, 1]), represents the distance of each point visible to the light to the light itself.

\section{Radiance Injection}
In this render pass, we need to fill a 3D texture (called the radiance texture) with our `VPLs'. The VPLs represent the light being bounced off of geometry within the scene. The points at which this happens are precisely where the light hits the geometry, which is stored in the shadowmap. Therefore, this pass involves taking all points in the shadowmap and projecting the color of the corresponding geometry into the radiance texture.

The most straightforward way to accomplish this is to use a compute shader and launch one thread for each pixel in the shadowmap. Each thread will read the depth value at its respective pixel, project it from light space to world space, look up the corresponding color value in the voxel texture, and finally write it into the radiance texture.

It is important the shadowmap is rendered with a high enough resolution (relative to the voxel texture resolution) to ensure a smooth radiance injection. If the shadowmap resolution is too low, there will be gaps between voxels which cause lighting artifacts.

% TODO show lighting artifacts if shadowmap resolution too low
% TODO temporal filtering?

\section{Radiance Filtering}
With the highest level of detail of the radiance texture filled, the next step is creating the filtered representation. Each successive level is half the resolution of the previous level. Therefore, the maximum number of levels the texture may have is $log_2 (\max (\text{width}, \text{height})) + 1$.

OpenGL is able to automatically generate mipmaps for 3D textures by calling \verb#glGenMipmaps(GL_TEXTURE_3D)#, however manually filtering the textures using a compute shader turned out much faster for this application (how OpenGL calculates mipmaps is implementation defined, so the performance can vary between systems and drivers). To perform the filtering, a compute shader kernel is launched for each level of the radiance texture (not including the base level). The kernel is launched with one thread for each voxel in the current level. Each thread can be conceptually located at the corners of each voxel in the previous level, as shown in TODOODODO, and thus it averages the surrounding 8 values to compute the value of its respective voxel. In other words, this is a 3D convolution with a 2x2x2 kernel with even weights.

% TODO probably remove this, or maybe future work?
% TODO or do it and then it's current work (and novel)
The other benefit to manually filtering is different methods can be used. For example, adjusting the size or weights of the filtering kernel could result in different effects.

\section{Shading}
% TODO do i need to show code/go step by step here?
The final shading step takes all of the previously generated information (voxel texture, filtered radiance texture, shadowmaps) and renders the scene. Direct lighting is computed from multiple lights and uses the simple Blinn-Phong shading model. Indirect lighting is computed using voxel cone tracing.

\subsection{Direct Lighting}
We calculate direct lighting by iterating over each light source in the scene and summing all lighting contributions. We compute diffuse and specular lighting as follows:
diffuse: material.diffuseColor * light.color * max(0, dot(N, L))
specular: material.specularColor * light.color * pow(max(0, dot(H, L)), shininess)


% TODO definitely gonna want some pictures
% TODO present integral as going backwards? (want integral -> inf many rays -> break into discrete chunks w/ weights)
\subsection{Indirect Lighting (Voxel Cone Tracing)}
To compute the indirect lighting at a given point, we perform the voxel cone tracing step. Recall that to compute indirect light we are effectively approximating a surface integral over a hemisphere. To do this perfectly would require infinitely many cones. Instead, we use six cones: five for diffuse light and one for a specular highlight.

% TODO multiple cones + directions, weights and math supporting it
% TODO FILL IN APERTURE (from vct settings) AND ANGLE
% TODO Gaussian distribution for cone weights
For the diffuse cones we chose an aperture of $xx\degree$s with one oriented in the direction of the surface normal and the others evenly distributed around the normal and tilted up at an angle of $xx\degree$s. We also assign weights to each cone based on a uniform distribution. Note that in order to compute the orientation of each cone in world space we must multiply the chosen cone directions by the TBN matrix.

% TODO use Simon's value to determine aperture?
% TODO make sure to define these somewhere
The specular cone is oriented in the direction of the reflection vector, which is calculated as $-V - 2 * dot(N, -V) * N$. The aperture is derived from the material's shininess according to the following formula: $TODO$.

% TODO place this somewhere...
The radiance texture is sampled at varying levels of detail in order to approximate the amount of indirect light at that point. At it's core, voxel cone tracing is the combination of two fairly simple ideas: raymarching and mipmapping.

Raymarching is a well-known technique that involves sampling values along a ray at regular intervals, or steps. The result of a single raymarch is the accumulation of sampled values along the ray and is often blended based on opacity. The raymarching itself continues until either a set number of steps is calculated or when the accumulated opacity becomes close to 1.

Mipmapping involves keeping multiple detail levels of the same source data. The base level is the highest resolution available and each successive level is downscaled. While this does require increased memory usage, it drastically increases performance as it improves GPU cache locality. For voxel cone tracing, the filtered radiance texture is the mipmapped texture.

% TODO  add math n shit yo
Therefore, voxel cone tracing is simply raymarching through a mipmapped texture. In order to determine which miplevel to sample from, the idea of cone tracing is introduced. Instead of having a simple ray, we imagine a cone with a particular aperture (angle). Then, the height of the cone is analagous to a ray's length and the size (diameter) of the cone's base grows as the height increases. We can then map the diameter to a level of detail.

Combining this all together, we sample from the radiance texture at a level of detail related to the height of the cone at our sampling point. In this way, samples close to the start of the cone come from higher detailed data and samples far from the start come from lower detailed data. This allows the voxel cone tracing to gather both high frequency and low frequency details of the indirect light.

\section{Voxel Warping}
% TODO deriving spline, why need constraints, increasing viewport resolution
The goal of voxel warping is to vary the density of the voxel resolution according to some metric. In particular, this allows the voxel density to be higher near the camera and then smoothly fall off for objects further away.

Recall from voxelization that in order to determine the location in the voxel texture for a given fragment we perform a linear mapping from world space to image space. As an intermediate step, we have the voxel position in texture space (in the range $[0, 1]$). Thus, for example, if the position along the $y$ axis is 0 it will go into the bottom of the voxel texture and if 1 will go into the top part of the voxel texture. Now, consider mapping the coordinate in texture space to so-called warp space, which is also in the range $[0, 1]$. Without any modifications, this is a straight line with a slope of 1, representing a linear `warping' function. If we let $w_{linear}: [0, 1] \rightarrow [0, 1]$ be the warping function and $x \in [0, 1]$ be the coordinate in texture space then we just have $w_{linear}(x) = x$. Now consider a different function, $w_{logistic}(x) = \frac{1}{1 + e^{-x}}$ (this is basic logistic function, a type of ``S''-shaped curve). Notice how towards the edges the slope decreases and in the middle the slope is greater than one. If we draw lines up and over from the curve for $x = 0.4$ and $x = 0.6$ we see that the range of $w_{logistic}(x)$ is greater than that of $w_{linear}(x)$. In other words, the positions within those particular $x$ values `take up more space' in the voxel texture. Likewise, towards the ends, the slope decreases towards zero and thus takes up less space. This achieves the goal of varying the voxel density resolution according to a simple function. We also see that the slope, $w'(x)$, represents the voxel density: if $w'(x) = 1$, the density is the same as the linear mapping; if $w'(x) > 1$, the density is greater than the linear mapping; if $w'(x) < 1$, the density is less than the linear mapping.
% TODO lots of pictures, show  functions and lines; start off with cubic instead?

The ideal warping function increases voxel density near the camera and decreases voxel density further away. The logistic function provided does this, however there are some issues. Recall from the Voxelization section that the scene is rendered with a viewport of dimensions equal to that of the voxel texture. The discretized fragment positions will therefore all be in step sizes corresponding to this viewport resolution. When the warp function is applied where $w'(x) > 1$, we can therefore run into issues where adjacent fragments will `skip' a position in the voxel texture. In essence, the voxel fragments are not generated with fine enough resolution to smoothly transition after being warped (this is similar to the concept of the Nyquist Frequency, where we are not sampling the signal at a high enough rate). To resolve this, the scene must be voxelized with a viewport resolution scaled by the maximum derivative of $w(x)$. In design terms, this means choosing a warping function with a steep slope will result in needing to voxelize the scene with a larger viewport resolution, which can hurt performance. % TODO show cracks and the heatmap

Using the logistic function as the warping function also has another issue: the slope at the ends approaches zero. This leads to a large portion of the voxelized region ending up in relatively few voxels, which diminishes the accuracy of the voxelization greatly. % TODO show this
Instead, we want to place a lower bound on $w'(x)$. The solution to this is to use a cubic spline, i.e.\ $w(x) = a + bx + cx^2 + dx^3$. Then, choosing 0.25 as the desired end slope, we use the  constraints $w(0) = 0, w(1) = 1, \text{ and } w'(0) = w'(1) = 0.25$ and solve for the variables $a$, $b$, $c$, and $d$. This gives the warping function we use: $w(x) = \ldots$ % TODO

% TODO imply that 0.5 is close to camera? idk
% TODO go over how this doesnt mess up voxel cone tracing or filtering and stuff
 
% TODO do i need this?
\section{Optimizations}
\subsection{Depth Prepass}
With voxel cone tracing, each fragment shader invocation is quite expensive. If we have overlapping objects in the scene then fragments will be shaded for each pixel, but only one will be the final color. To ensure we only shade the final fragment, we rasterize the scene and only write the depth value (similar to shadow mapping). This operation is extremely quick. Now, the depth buffer contains only the depth of the fragment that will end up on the screen. When the full shading pass is performed all fragments that fail the depth test can be immediately discarded, avoiding the expensive voxel cone tracing.

% TODO custom filtering, try raymarching optimizations from nvidia blog, dds textures