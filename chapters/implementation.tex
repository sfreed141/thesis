\chapter{Implementation}

At its core, the application is a standard forward rendering engine supporting multiple lights, shadow mapping, and normal mapping---in addition, of course, to full real-time dynamic global illumination. The scene to be rendered is composed of one or more actors, which are simply meshes loaded from the generic Wavefront OBJ file format. Actors are also capable of rigid body animations.

The rendering pipeline can be broken down into several render passes, each of which will be explained in its corresponding section. In general, the main steps taken are generating a voxelized representation of the scene, creating a filtered representation of light from the voxels and light sources, and finally shading the scene.

The application itself is written in C++11 and uses OpenGL as the graphics API. In order to make use of modern graphics features like compute shaders and direct state access we require OpenGL version 4.5 (released in 2014). The project uses CMake as its build system and has been tested on Linux and Windows.

% TODO include code snippets, API calls, diagrams
%TODO name the 3D texture (and refer to it in monospace)?
\section{Voxelization}
The goal of voxelization is to create a sparse 3D representation of the geometry in the scene. The information associated with each voxel is stored in two 3D textures: one for color and opacity and another for surface normal. We refer to these textures as \texttt{voxelColor} and \texttt{voxelNormal}, respectively. Each texture has an internal format of \texttt{RGBA16f} or \texttt{RGBA8}, depending on hardware capabilities. The resolution of the textures are configurable at runtime, although the default is $256^3$.

The extents of our voxelization volume---the area in world space that will end up inside the voxel textures---is also configurable and by default is set to enclose the entire scene. This volume can be static or can track the camera (with the camera being at the center of the textures). In addition, to prevent some issues with temporal artifacts, the voxel textures are snapped to a discrete grid corresponding to the size of each voxel cell in world space.

% TODO scene extents, midpoint and tracking, computing voxel texture coordinates, choosing axis, diagram of scene extents and converting to NDC
% TODO cite GPU Pro 4 and imageAtomicMax
The implementation utilizes the GPU rasterization pipeline and is based on~\cite{crassin2012octree}. The voxelization is performed completely in a single render pass. The basic idea is to rasterize the scene such that each fragment generated by the GPU corresponds to a single voxel in the 3D texture.
One of the main challenges with voxelization is ensuring there are no holes or cracks in the resulting grid. Two techniques are used to mitigate this (and will be covered in-depth in the following paragraphs). First, for each triangle we project along the triangle's normal's dominant axis. This maximizes the number of fragments generated for a given triangle. Second, we perform conservative raterization. Normally, the GPU only generates a fragment for a given pixel if a triangle overlaps the center of the corresponding pixel. This results in cracks in the voxelized grid since even if a triangle is inside a voxel it may not be rasterized.

Before the draw call, the necessary matrices to project the geometry into the voxelization volume are created. The projection matrix is orthographic, with bounds corresponding to the extents of the voxelization volume (shown in Figure TODO). In order to project along each triangle's dominant axis, we also need three view matrices: TODO. The final matrices used to transform a point in world space to a point in the voxel textures is the multiplication of the orthographic matrix with one of the view matrices. Also, the voxel textures are bound as \texttt{image3D} objects (as textures are read-only in shaders). Following from that, since the framebuffer is not being written to, color writing is disabled (TODO show code). Lastly, depth testing and depth writes are also disabled.

Starting in the vertex shader, the only operation performed is transforming the object from object space to world space using its appropriate model matrix. In the geometry shader we take a single triangle in world space, transform it based on its dominant axis, and output the resulting triangle in clip space. The triangle's dominant axis is determined by finding the largest component of its normal (the absolute value)\footnote{Finding the dominant axis is really finding the maximum dot product of each axis with the normal, but since we use the standard basis this reduces to just finding the largest component of the normal.}. We then apply the appropriate projection matrix according to the dominant axis and output the resulting triangle to be rasterized. In the fragment shader, each fragment's position in Normalized Device Coordinates can be used to determine its position within the voxel textures. Note, however, that the components of each fragment's position need to be realigned due to projecting along different axes (TODO code snippet?). Finally, the voxel information can be written into the voxel textures, which are bound as \texttt{image3D} objects in the shader.

However there is still one more issue: multiple voxel fragments can be mapped to the same cell in the voxel textures. Without any synchronization, the final voxel values are not deterministic and can result in temporal artifacts. To solve this, we average the resulting color value using atomic operations\footnote{Other approaches to this problem (with various tradeoffs) can be used (Doghramachi uses an atomic max operation on a custom defined metric in order to avoid needing to perform an average~\cite{doghramachi2013rasterized}). These are generally used to combat performance issues or hardware limitations.}.
% TODO should explain rgba8 + faking atomic avg vs rgba16f + atomicAdd + compute shader

% TODO need to work this in better. subsection?
% TODO should expand on problem more
\subsection{Conservative Rasterization}
As mentioned previously, we need conservative rasterization to minimize holes and cracks in the voxelization. To achieve this, we use an MSAA based approach~\cite{takeshige2015basics}. MSAA (multi-sample anti-aliasing) is a technique used for smoothing out ragged edges (aliasing) in a rasterized image. Instead of only sampling at one position inside the voxel, multiple samples are used (hence the name). These samples are typically distributed to maximize the possibility of a rasterized fragment being produced. For each sample point that overlaps with the triangle, a fragment is generated. The final value of the fragment (e.g.\ rgb color value of a pixel) is the averaged value of all the samples. For conservative rasterization, we only need one fragment to be generated, which can be configured with TODO. While the MSAA method of conservative rasterization is not perfect, it is cross-platform, simple to use, and efficient. Other notable methods would be manually dilating each triangle in a geometry shader (better quality but slower)~\cite{akenine2005conservative} or using a GPU vendor specific extension, like \verb#GL_NV_conservative_raster# (better quality but not cross-platform).

% TODO should implement the geometry shader based and then compare performance quality of all 3 (might need to do that indirect rendering thing too...)
% TODO go through shaders. NV_conservative_raster, MSAA based (CITE THIS), image of MSAA and the sampling, one fragment for msaa

% TODO code snippets, actual OpenGL names
\section{Shadow Mapping}
Shadow mapping is a well known technique used for efficiently creating shadows. Similar to the related works, the resulting shadow map is also treated as an RSM for radiance injection.

To generate a shadow map for a light, we render the scene from the light's perspective and gather the depth values of the fragments. Since we need the GPU to write to an arbitrary texture (the shadowmap), we create a framebuffer object (FBO) and attach the texture as a depth attachment\footnote{This general technique is referred to as render-to-texture.}. The FBO does not require any color attachments, as we are only interested in the depth. The matrix used to transform vertices from world space to light space is an orthographic projection matrix multiplied with a view matrix generated from the light. This matrix is often called the light view matrix. Since the GPU will automatically write depth values, the fragment shader can be completely empty. The resulting shadow map, which only contains floating point depth values (in the range [0, 1]), represents the distance of each point visible to the light to the light itself.

\section{Radiance Injection}
In this render pass, we fill a 3D texture with virtual point lights from the shadow map. Recall that the VPLs are light sources which represent the light being bounced off of geometry within the scene. Following from RSMs, the points at which this happens are precisely where the light hits the geometry, which is stored in the shadow map. Therefore, this pass involves taking all points in the shadow map and projecting the color of the corresponding geometry into the radiance texture.

The radiance texture, \texttt{voxelRadiance}, has dimensions equal to that of the other voxel textures. It has a format of \texttt{RGBA8} and stores a color value and opacity.

The most straightforward way to accomplish radiance injection from the shadow map is to use a compute shader and launch one thread for each pixel in the shadow map. Each thread first reads the depth value at its respective pixel, which can then be used to reconstruct the pixel's position in light space. Then, by using the inverse light view matrix, we project it from light space to world space. The world space position is used to calculate the voxel position, which is used to sample from the voxel textures and finally write the color into the radiance texture.

It is important the shadowmap is rendered with a high enough resolution (relative to the voxel texture resolution) to ensure a smooth radiance injection. If the shadowmap resolution is too low, there will be gaps between voxels which cause lighting artifacts. Note also that we do not need to be concerned with synchronization and atomic operations in the case that multiple threads access the same voxel position since we are only interested in the base color.

% TODO show lighting artifacts if shadowmap resolution too low
% TODO temporal filtering?

\section{Radiance Filtering}
With the highest level of detail of the radiance texture filled, the next step is creating the filtered representation. Each successive level is half the resolution of the previous level. Therefore, the maximum number of levels the texture may have is $log_2 (\max (\text{width}, \text{height})) + 1$.

% TODO timing difference between glgenmipmaps and compute shader
OpenGL is able to automatically generate mipmaps for 3D textures by calling \verb#glGenMipmaps(GL_TEXTURE_3D)#, however manually filtering the textures using a compute shader turned out much faster for this application (how OpenGL calculates mipmaps is implementation defined, so the performance can vary between systems and drivers). To perform the filtering, a compute shader kernel is launched for each level of the radiance texture (not including the base level). The kernel is launched with one thread for each voxel in the current level. Each thread can be conceptually located at the corners of each voxel in the previous level, as shown in TODOODODO, and averages the surrounding 8 values to compute the filtered value of its respective voxel. In other words, this is a 3D convolution with a 2x2x2 kernel with even weights.

% TODO probably remove this, or maybe future work?
% TODO or do it and then it's current work (and novel)
The other benefit to manually filtering is different methods can be used. For example, adjusting the size or weights of the filtering kernel could result in different effects.

\section{Shading}
% TODO do i need to show code/go step by step here?
The final shading step takes all of the previously generated information and renders the scene. Direct lighting is computed from multiple lights and uses the classic Blinn-Phong shading model~\cite{Phong:1975:ICG:360825.360839}. Indirect lighting is computed using voxel cone tracing. Normal mapping (similar to bump mapping~\cite{Blinn:1978:SWS:965139.507101}), shadow mapping, and post processing effects also occur here.

% TODO section  on normal mapping, shadow mapping, postprocessing?  or maybe in background? 

% TODO explain UBO offsets and such? (also should this be in background?)
Two important inputs for the fragment shader are material information and light information. The material information for each fragment is provided in a struct \texttt{Material}, uploaded as a uniform buffer object, and contains color and texture information. Lights are stored as an array of struct \texttt{Light}s and contain information associated with them such as light type (e.g.\ directional or point light), position, and color. The array is uploaded as a shader storage buffer object, which allows the array to have a dynamic size queryable at shader runtime.

\subsection{Direct Lighting}
We calculate direct lighting by iterating over each light source in the scene and summing all lighting contributions. We compute diffuse and specular lighting using the Blinn-Phong shading model as follows:
\begin{description}
    \item[diffuse:] \texttt{material.diffuseColor * light.color * max(0, dot(N, L))}
    \item[specular:] \texttt{material.specularColor * light.color * pow(max(0, dot(H, L)), shininess)}
\end{description}
% TODO should I expand on this in detail?

% TODO definitely gonna want some pictures
% TODO present integral as going backwards? (want integral -> inf many rays -> break into discrete chunks w/ weights)
\subsection{Indirect Lighting (Voxel Cone Tracing)}
To compute the indirect lighting at a given point, we perform the voxel cone tracing step. Recall that to compute indirect light we are effectively approximating a surface integral over a hemisphere. To do this perfectly would require infinitely many cones. Instead, we use six cones: five for diffuse light and one for a specular highlight.

% TODO should either explain vct in related works or right here before detailing cone properties
% TODO multiple cones + directions, weights and math supporting it
% TODO FILL IN APERTURE (from vct settings) AND ANGLE
% TODO Gaussian distribution for cone weights
For the diffuse cones we chose an aperture of $45^\circ$ with one cone oriented in the direction of the surface normal and the others evenly distributed around the normal and tilted up at an angle of $45^\circ$. We also assign weights to each cone based on a uniform distribution. Note that in order to compute the orientation of each cone in world space we must multiply the chosen cone directions by the TBN matrix, since the cone directions are defined in tangent space. To get the final diffuse contribution, the cone tracing is performed for each cone and the values are weighted and summed appropriately.

% TODO use Simon's value to determine aperture?
% TODO make sure to define these somewhere
The specular cone is oriented in the direction of the reflection vector, which is calculated as \texttt{-V - 2 * dot(N, -V) * N}, or by using the GLSL function \texttt{reflect()}. The aperture is derived from the material's shininess according to the following formula: $TODO$.

% TODO place this somewhere...
% TODO  add math n shit yo
The radiance texture is sampled at varying levels of detail in order to approximate the amount of indirect light at that point. At it's core, voxel cone tracing is raymarching through a mipmapped texture. In order to determine which miplevel to sample from, the idea of cone tracing is introduced. Instead of having a simple ray, we imagine a cone with a particular aperture (angle). Then, the height of the cone is analagous to a ray's length and the size (diameter) of the cone's base grows as the height increases. We can then map the diameter to a level of detail.

Combining this all together, we sample from the radiance texture at a level of detail related to the height of the cone at our sampling point. In this way, samples close to the start of the cone come from higher detailed data and samples far from the start come from lower detailed data. This allows the voxel cone tracing to gather both high frequency and low frequency details of the indirect light.

% TODO go over all the vct settings
The implementation of a single cone tracing instance is done entirely within a fragment shader. The main inputs are the radiance texture, the position to start tracing from, and the direction in which to trace. There are also several configurable parameters that influence the cone tracing. We have \texttt{steps}, the maximum number of samples to take; \texttt{bias}, the initial offset from the starting position (in order to avoid self illumination); \texttt{coneAngle}, the angle of the cone; \texttt{coneHeight}, the starting height of the cone; and \texttt{lodOffset}, a constant offset used when determining the level of the radiance texture to sample from. % TODO finish explaining the vct loop

\section{Voxel Warping}
% TODO deriving spline, why need constraints, increasing viewport resolution
The goal of voxel warping is to vary the density of the voxel resolution according to some metric. In particular, this allows the voxel density to be higher near the camera and then smoothly fall off for objects further away.

\subsection{The Warping Function}
Recall from voxelization that in order to determine the location in the voxel texture for a given fragment we perform a linear mapping from world space to image space. As an intermediate step, we have the voxel position in texture space (in the range $[0, 1]$). Thus, for example, if the position along the $y$ axis is 0 it will go into the bottom of the voxel texture and if 1 will go into the top part of the voxel texture. Now, consider mapping the coordinate in texture space to so-called warp space, which is also in the range $[0, 1]$. Without any modifications, this is a straight line with a slope of 1, representing a linear `warping' function. If we let $w_{linear}: [0, 1] \rightarrow [0, 1]$ be the warping function and $x \in [0, 1]$ be the coordinate in texture space then we just have $w_{linear}(x) = x$. Of course, this warping function does not do anything useful. For that, we need a different, nonlinear, warping function.

\subsection{Nonlinear Warping}
Now consider a different function, $w_{logistic}(x) = \frac{1}{1 + e^{-x}}$ (this is basic logistic function, a type of ``S''-shaped curve). Notice how towards the edges the slope decreases and in the middle the slope is greater than one. If we draw lines up and over from the curve for $x = 0.4$ and $x = 0.6$ we see that the range of $w_{logistic}(x)$ is greater than that of $w_{linear}(x)$. In other words, the positions within those particular $x$ values `take up more space' in the voxel texture. Likewise, towards the ends, the slope decreases towards zero and thus takes up less space. This achieves the goal of varying the voxel density resolution according to a simple function. We also see that the slope, $w'(x)$, represents the voxel density: if $w'(x) = 1$, the density is the same as the linear mapping; if $w'(x) > 1$, the density is greater than the linear mapping; if $w'(x) < 1$, the density is less than the linear mapping.
% TODO lots of pictures, show  functions and lines; start off with cubic instead?

The ideal warping function increases voxel density near the camera and decreases voxel density further away. The logistic function provided does this, however there are some issues. Recall from the Voxelization section that the scene is rendered with a viewport of dimensions equal to that of the voxel texture. The discretized fragment positions will therefore all be in step sizes corresponding to this viewport resolution. When the warp function is applied where $w'(x) > 1$, we can therefore run into issues where adjacent fragments will `skip' a position in the voxel texture. In essence, the voxel fragments are not generated with fine enough resolution to smoothly transition after being warped (this is similar to the concept of the Nyquist Frequency, where we are not sampling the signal at a high enough rate). To resolve this, the scene must be voxelized with a viewport resolution scaled by the maximum derivative of $w(x)$. In design terms, this means choosing a warping function with a steep slope will result in needing to voxelize the scene with a larger viewport resolution, which can hurt performance. % TODO show cracks and the heatmap

Using the logistic function as the warping function also has another issue: the slope at the ends approaches zero. This leads to a large portion of the voxelized region ending up in relatively few voxels, which diminishes the accuracy of the voxelization greatly. % TODO show this
Instead, we want to place a lower bound on $w'(x)$. The solution to this is to use a cubic spline, i.e.\ $w(x) = a + bx + cx^2 + dx^3$. Then, choosing 0.25 as the desired end slope, we use the constraints $w(0) = 0, w(1) = 1, \text{ and } w'(0) = w'(1) = 0.25$ and solve for the variables $a$, $b$, $c$, and $d$. This gives the warping function we use: $w(x) = \ldots$ % TODO

% TODO imply that 0.5 is close to camera? idk
% TODO go over how this doesnt mess up voxel cone tracing or filtering and stuff
 
% TODO do i need this?
\section{Optimizations}
\subsection{Depth Prepass}
With voxel cone tracing, each fragment shader invocation is quite expensive. If we have overlapping objects in the scene then fragments will be shaded for each pixel, but only one will be the final color. To ensure we only shade the final fragment, we have a prepass which rasterizes the scene and only writes the depth value. This operation is extremely quick. Now, the depth buffer contains only the depth of the fragment that will end up on the screen. When the full shading pass is performed all fragments that fail the depth test can be immediately discarded, avoiding the expensive voxel cone tracing.
% TODO give before/after improvement

\subsection{Compressed Textures with Pregenerated Mipmaps}
% TODO ?

% TODO custom filtering, try raymarching optimizations from nvidia blog